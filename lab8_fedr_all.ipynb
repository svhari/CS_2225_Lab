{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e74d79-6d70-4724-ae65-d0fb7b371c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab8_fedr_all.ipynb\n",
    "# 22 Jun 2025\n",
    "# Lab 8: Feature Engineering and Dimensionality Reduction\n",
    "# Dowload code from : https://github.com/svhari/CS_2225_Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973e211-6625-466e-b915-fde25680bb16",
   "metadata": {},
   "source": [
    "# **Lab 8: Feature Engineering and Dimensionality Reduction**\n",
    "\n",
    "\n",
    "\n",
    "## Program 1: Feature Engineering Techniques \n",
    "1. **Feature Engineering Practice**\n",
    "   - Load the titanic dataset from \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "   - Apply the following feature engineering techniques:\n",
    "     - Data loading and inspection\n",
    "     - Missing value handling\n",
    "     - Categorical variable encoding\n",
    "     - Feature scaling/normalization\n",
    "     - Feature creation\n",
    "     - Result visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbac6d-8796-4e20-bcb3-ddf46a17bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as pltS\n",
    "\n",
    "# ======================================================================\n",
    "# 1. DATA LOADING AND INITIAL INSPECTION\n",
    "# ======================================================================\n",
    "print(\"STEP 1: DATA LOADING AND INSPECTION\".center(70, '='))\n",
    "\n",
    "# Load the Titanic dataset from a public URL\n",
    "# Note: This dataset contains information about passengers and their survival\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display basic dataset information\n",
    "print(\"\\nOriginal Data (First 5 Rows):\")\n",
    "print(data.head())  # Shows column names and sample values\n",
    "\n",
    "# Check for missing values - crucial for data quality assessment\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(data.isnull().sum())  # Counts nulls per column\n",
    "\n",
    "\"\"\"\n",
    "KEY OBSERVATIONS:\n",
    "- 'age' has missing values (continuous numerical feature)\n",
    "- 'embarked' has few missing values (categorical feature)\n",
    "- 'cabin' has many missing values (will be dropped)\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# 2. MISSING VALUE HANDLING\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 2: MISSING VALUE IMPUTATION\".center(70, '='))\n",
    "\n",
    "# Strategy for numerical features (age, fare): Median imputation\n",
    "# Why median? Robust to outliers compared to mean\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data[['age', 'fare']] = imputer.fit_transform(data[['age', 'fare']])\n",
    "\n",
    "# Strategy for categorical feature (embarked): Mode imputation\n",
    "# Why mode? Most frequent value is sensible for categories\n",
    "data['embarked'] = data['embarked'].fillna(data['embarked'].mode()[0])\n",
    "\n",
    "# Dropping columns:\n",
    "# - 'cabin' has too many missing values (>70%)\n",
    "# - 'embark_town' is redundant with 'embarked'\n",
    "data.drop(['cabin', 'embark_town'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "print(\"\\nAfter Missing Value Handling:\")\n",
    "print(data.isnull().sum())  # Verify no missing values remain\n",
    "\n",
    "# ======================================================================\n",
    "# 3. CATEGORICAL VARIABLE ENCODING\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: CATEGORICAL ENCODING\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "WHY ONE-HOT ENCODING?\n",
    "- Convert categorical text data to numerical format\n",
    "- 'drop=\"first\"' removes one column to avoid dummy variable trap\n",
    "- 'sparse_output=False' returns array instead of sparse matrix\n",
    "\"\"\"\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(data[['sex', 'embarked']])\n",
    "\n",
    "# Get automatically generated column names\n",
    "encoded_df = pd.DataFrame(encoded_features, \n",
    "                         columns=encoder.get_feature_names_out(['sex', 'embarked']))\n",
    "\n",
    "print(\"\\nEncoded Features:\")\n",
    "print(encoded_df.head())\n",
    "\n",
    "# ======================================================================\n",
    "# 4. FEATURE SCALING\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: FEATURE SCALING\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "WHY STANDARD SCALER?\n",
    "- Transforms features to have mean=0 and std=1\n",
    "- Important for distance-based algorithms (KNN, SVM, PCA)\n",
    "- Helps models converge faster (neural networks, logistic regression)\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[['age', 'fare']])\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=['age_scaled', 'fare_scaled'])\n",
    "\n",
    "print(\"\\nScaled Features Description:\")\n",
    "print(scaled_df.describe())  # Show mean≈0 and std≈1\n",
    "\n",
    "# ======================================================================\n",
    "# 5. FEATURE CREATION\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 5: FEATURE CREATION\".center(70, '='))\n",
    "\n",
    "# Create family_size by combining siblings/spouses + parents/children\n",
    "# Adding 1 to include the passenger themselves\n",
    "data['family_size'] = data['sibsp'] + data['parch'] + 1\n",
    "\n",
    "# Create is_alone flag (boolean converted to 0/1)\n",
    "data['is_alone'] = (data['family_size'] == 1).astype(int)\n",
    "\n",
    "print(\"\\nNew Features Sample:\")\n",
    "print(data[['sibsp', 'parch', 'family_size', 'is_alone']].head())\n",
    "\n",
    "# ======================================================================\n",
    "# 6. FINAL DATASET ASSEMBLY\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 6: FINAL DATASET\".center(70, '='))\n",
    "\n",
    "# Combine all processed features\n",
    "final_data = pd.concat([\n",
    "    data[['pclass', 'family_size', 'is_alone']],  # Original numeric features\n",
    "    encoded_df,                                   # Encoded categoricals\n",
    "    scaled_df,                                    # Scaled numeric features\n",
    "    data['survived']                              # Target variable\n",
    "], axis=1)\n",
    "\n",
    "print(\"\\nFinal Processed Data (First 5 Rows):\")\n",
    "print(final_data.head())\n",
    "\n",
    "# ======================================================================\n",
    "# 7. VISUALIZATION\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 7: VISUALIZATION\".center(70, '='))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original vs Scaled Age Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data['age'], bins=20, color='blue', alpha=0.7)\n",
    "plt.title('Original Age Distribution')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(final_data['age_scaled'], bins=20, color='red', alpha=0.7)\n",
    "plt.title('Standard Scaled Age Distribution')\n",
    "plt.xlabel('Standardized Age Units')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "The scaling maintains the distribution shape but changes the units\n",
    "- Left: Original age values in years\n",
    "- Right: Standard deviations from the mean age\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344cb26-61c9-4e34-a2c0-ef681a85ec09",
   "metadata": {},
   "source": [
    "## Program 2: Dimensionality Reduction\n",
    "\n",
    "\n",
    "\n",
    "Demonstrate the two fundamental dimensionality reduction techniques:\n",
    "1. Principal Component Analysis (PCA) - Linear method\n",
    "2. t-Distributed Stochastic Neighbor Embedding (t-SNE) - Nonlinear method\n",
    "\n",
    "Demonstrate the following:\n",
    "- Data standardization requirements\n",
    "- Variance explanation in PCA\n",
    "- Perplexity in t-SNE\n",
    "- Visualization of high-D data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ccbb2-3532-42be-aada-022d2738d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ======================================================================\n",
    "# 1. DATA LOADING AND PREPARATION\n",
    "# ======================================================================\n",
    "print(\"STEP 1: DATA LOADING AND PREP\".center(70, '='))\n",
    "\n",
    "# Load Wine Recognition dataset - 13 chemical measurements of 3 wine types\n",
    "wine = load_wine()\n",
    "X = wine.data       # Features (178 samples × 13 features)\n",
    "y = wine.target     # Target classes (3 wine cultivars)\n",
    "feature_names = wine.feature_names\n",
    "\n",
    "print(f\"\\nDataset Shape: {X.shape}\")\n",
    "print(f\"Feature Names:\\n{feature_names}\")\n",
    "print(f\"\\nTarget Classes: {wine.target_names}\")\n",
    "\n",
    "# Standardize the data - CRUCIAL for PCA\n",
    "\"\"\"\n",
    "WHY STANDARDIZE?\n",
    "- PCA is sensitive to feature scales\n",
    "- Variables with larger ranges would dominate\n",
    "- We want each feature to contribute equally\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ======================================================================\n",
    "# 2. PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 2: PRINCIPAL COMPONENT ANALYSIS\".center(70, '='))\n",
    "\n",
    "# Reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nExplained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total Explained Variance: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- PC1 explains 36% of variance\n",
    "- PC2 explains 19% of variance\n",
    "- Together they capture 55% of total variance\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# 3. T-DISTRIBUTED STOCHASTIC NEIGHBOR EMBEDDING (t-SNE)\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: t-SNE DIMENSIONALITY REDUCTION\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "t-SNE PARAMETERS:\n",
    "- perplexity: Balances local/global structure (typically 5-50)\n",
    "- n_iter: Optimization iterations (default 1000)\n",
    "- random_state: For reproducibility\n",
    "\"\"\"\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# ======================================================================\n",
    "# 4. VISUALIZATION COMPARISON\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: VISUALIZATION\".center(70, '='))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original First Two Features\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel(feature_names[0])  # Alcohol\n",
    "plt.ylabel(feature_names[1])  # Malic acid\n",
    "plt.title(\"Original Features (First Two)\")\n",
    "plt.colorbar(ticks=[0, 1, 2], label='Wine Class')\n",
    "\n",
    "# PCA Projection\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('PC1 (36% Variance)')\n",
    "plt.ylabel('PC2 (19% Variance)')\n",
    "plt.title(\"PCA Projection\")\n",
    "plt.colorbar(ticks=[0, 1, 2], label='Wine Class')\n",
    "\n",
    "# t-SNE Projection\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.title(\"t-SNE Projection (perplexity=30)\")\n",
    "plt.colorbar(ticks=[0, 1, 2], label='Wine Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "# 5. PCA VARIANCE ANALYSIS\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 5: PCA VARIANCE ANALYSIS\".center(70, '='))\n",
    "\n",
    "# Fit PCA with all components to analyze variance\n",
    "pca_full = PCA().fit(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_), 'o-')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(np.cumsum(pca_full.explained_variance_ratio_) >= 0.95) + 1\n",
    "print(f\"\\nComponents needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "\"\"\"\n",
    "KEY INSIGHTS:\n",
    "1. PCA provides a linear projection that maximizes variance\n",
    "2. t-SNE reveals nonlinear patterns and clusters\n",
    "3. First two original features show poor separation\n",
    "4. About 8 components needed to capture 95% variance\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8ea41-b70b-40ef-ba00-a37ebf51951c",
   "metadata": {},
   "source": [
    "## Program 3: Feature Selection \n",
    "\n",
    "Compares the three approaches to feature selection:\n",
    "\n",
    "1. Filter Methods (Mutual Information) - Fast, model-agnostic\n",
    "2. Wrapper Methods (RFE) - Uses model performance\n",
    "3. Embedded Methods (L1 Regularization) - Built into model training\n",
    "\n",
    "(Each method has different computational costs and suitability scenarios.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ce01f-46e4-45f5-8e0f-87b756c1be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import (SelectKBest, mutual_info_classif,\n",
    "                                     RFE, SelectFromModel)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ======================================================================\n",
    "# 1. DATA LOADING AND PREPARATION\n",
    "# ======================================================================\n",
    "print(\"STEP 1: DATA LOADING AND PREP\".center(70, '='))\n",
    "\n",
    "# Load Breast Cancer Wisconsin dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "\n",
    "print(f\"\\nDataset Shape: {X.shape}\")\n",
    "print(f\"Target Classes:\\nMalignant: {sum(y==0)}\\nBenign: {sum(y==1)}\")\n",
    "\n",
    "# Split into train/test sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize data - important for regularized models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ======================================================================\n",
    "# 2. FEATURE SELECTION METHODS\n",
    "# ======================================================================\n",
    "\n",
    "def apply_feature_selection(method_name, selector, X_train, X_test):\n",
    "    \"\"\"Helper function to apply feature selection consistently\"\"\"\n",
    "    # Fit to training data\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    \n",
    "    # Transform test data using the same selector\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    if hasattr(selector, 'get_support'):\n",
    "        selected_features = feature_names[selector.get_support()]\n",
    "    elif hasattr(selector, 'estimator_'):  # For SelectFromModel\n",
    "        selected_features = feature_names[selector.estimator_.coef_.ravel() != 0]\n",
    "    \n",
    "    return X_train_selected, X_test_selected, selected_features\n",
    "\n",
    "# Mutual Information\n",
    "print(\"\\nApplying Mutual Information...\")\n",
    "selector_mi = SelectKBest(mutual_info_classif, k=5)\n",
    "X_train_mi, X_test_mi, features_mi = apply_feature_selection(\n",
    "    \"Mutual Information\", selector_mi, X_train_scaled, X_test_scaled)\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "print(\"\\nApplying Recursive Feature Elimination...\")\n",
    "model_rfe = LogisticRegression(max_iter=10000, random_state=42)\n",
    "selector_rfe = RFE(model_rfe, n_features_to_select=5, step=1)\n",
    "X_train_rfe, X_test_rfe, features_rfe = apply_feature_selection(\n",
    "    \"RFE\", selector_rfe, X_train_scaled, X_test_scaled)\n",
    "\n",
    "# L1 Regularization\n",
    "print(\"\\nApplying L1 Regularization...\")\n",
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', \n",
    "                            max_iter=10000, random_state=42)\n",
    "selector_l1 = SelectFromModel(model_l1)\n",
    "X_train_l1, X_test_l1, features_l1 = apply_feature_selection(\n",
    "    \"L1\", selector_l1, X_train_scaled, X_test_scaled)\n",
    "\n",
    "# ======================================================================\n",
    "# 3. MODEL EVALUATION\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: MODEL EVALUATION\".center(70, '='))\n",
    "\n",
    "def evaluate_model(X_train, X_test, method_name):\n",
    "    \"\"\"Train and evaluate logistic regression with selected features\"\"\"\n",
    "    model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Ensure test data has same number of features\n",
    "    assert X_train.shape[1] == X_test.shape[1], \\\n",
    "        f\"Feature mismatch: train {X_train.shape[1]} vs test {X_test.shape[1]}\"\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"- Features: {X_train.shape[1]}\")\n",
    "    print(f\"- Accuracy: {accuracy:.3f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "evaluate_model(X_train_scaled, X_test_scaled, \"All Features\")\n",
    "evaluate_model(X_train_mi, X_test_mi, \"Mutual Information\")\n",
    "evaluate_model(X_train_rfe, X_test_rfe, \"RFE\")\n",
    "evaluate_model(X_train_l1, X_test_l1, \"L1 Regularization\")\n",
    "\n",
    "# ======================================================================\n",
    "# 4. FEATURE IMPORTANCE VISUALIZATION\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: FEATURE IMPORTANCE VISUALIZATION\".center(70, '='))\n",
    "\n",
    "# Random Forest for comparison\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get importances and sort\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.ylabel(\"Relative Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "# 5. METHOD COMPARISON\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 5: METHOD COMPARISON\".center(70, '='))\n",
    "\n",
    "# Create comparison table\n",
    "methods = {\n",
    "    \"Mutual Information\": set(features_mi),\n",
    "    \"RFE\": set(features_rfe),\n",
    "    \"L1\": set(features_l1),\n",
    "    \"RF Top5\": set(feature_names[indices][:5])\n",
    "}\n",
    "\n",
    "# Print agreement between methods\n",
    "print(\"\\nFeature Selection Agreement:\")\n",
    "comparison = pd.DataFrame(index=feature_names)\n",
    "for method, features in methods.items():\n",
    "    comparison[method] = [feature in features for feature in feature_names]\n",
    "\n",
    "print(comparison.astype(int))\n",
    "\n",
    "# Calculate overlaps\n",
    "print(\"\\nPairwise Method Agreements:\")\n",
    "for name1 in methods:\n",
    "    for name2 in methods:\n",
    "        if name1 < name2:  # Avoid duplicate comparisons\n",
    "            overlap = len(methods[name1] & methods[name2])\n",
    "            print(f\"{name1} & {name2}: {overlap} common features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f03eb-d0db-459a-8d53-9056077ad172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
