{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57a27a-dd30-4197-ab9f-d2dfeee2041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab9_k_means_all.ipynb\n",
    "# 22 Jun 2025\n",
    "# Lab: K-Means Clustering\n",
    "# Dowload code from : https://github.com/svhari/CS_2225_Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde69947-d22f-41e2-9d62-8db6a68a5f1d",
   "metadata": {},
   "source": [
    "# **Lab 9: K-Means Clustering**\n",
    "\n",
    "## **Lab Objectives**\n",
    "1. Understand the fundamentals of **K-Means Clustering**, an unsupervised learning algorithm.S\n",
    "2. Learn how to **choose the optimal number of clusters (K)** using the **Elbow Method** and **Silhouette Analysis**.\n",
    "3. Implement K-Means clustering in Python using **scikit-learn**.\n",
    "4. Visualize clusters and interpret results.\n",
    "\n",
    "---\n",
    "\n",
    "# **Program 1: Basic K-Means Clustering with Synthetic Data**\n",
    "### **Objective**: Implement K-Means on a synthetic dataset and visualize clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9d500-4ef9-434d-ac96-f0b8a3a2972f",
   "metadata": {},
   "source": [
    "### **Lab Exercises**\n",
    "1. **Demonstrates the basic implementation of K-Means clustering on a synthetic dataset.**\n",
    "\n",
    "   - Generate synthetic data with clear clusters\n",
    "   - Apply K-Means clustering\n",
    "   - Apply K-Means clustering\n",
    "\n",
    "3. **Experiment with Different K Values**  \n",
    "   - Modify Program 1 to test `K=2,4,5`. How do clusters change?  \n",
    "   - Which `K` seems most appropriate?  \n",
    "\n",
    "4. **Impact of Initialization**  \n",
    "   - Compare `init='random'` vs `init='k-means++'` in Program 1.  \n",
    "   - Does the algorithm converge faster with `k-means++`?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd4625-c932-43ef-bebf-119bbcb89db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ======================================================================\n",
    "# 1. GENERATE SYNTHETIC DATA\n",
    "# ======================================================================\n",
    "print(\"STEP 1: GENERATE SYNTHETIC DATA\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "WHY SYNTHETIC DATA?\n",
    "- We can control the number of clusters and their distribution.\n",
    "- Helps in understanding how K-Means works before applying it to real-world data.\n",
    "\"\"\"\n",
    "\n",
    "# Generate 300 samples with 3 clusters\n",
    "X, y = make_blobs(\n",
    "    n_samples=300,          # Number of data points\n",
    "    centers=3,             # Number of clusters\n",
    "    cluster_std=0.8,       # Standard deviation of clusters (controls spread)\n",
    "    random_state=42        # Ensures reproducibility\n",
    ")\n",
    "\n",
    "print(f\"\\nData shape: {X.shape}\")  # Should be (300, 2) → 300 samples, 2 features\n",
    "\n",
    "# Plot the original data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c='blue', s=50, alpha=0.6)\n",
    "plt.title(\"Original Data (Unlabelled)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "# 2. SCALING THE DATA\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 2: STANDARDIZE FEATURES\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "WHY SCALING?\n",
    "- K-Means uses Euclidean distance → sensitive to feature scales.\n",
    "- Features on different scales can bias clustering.\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ======================================================================\n",
    "# 3. APPLY K-MEANS CLUSTERING\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: APPLY K-MEANS\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "K-MEANS PARAMETERS:\n",
    "- n_clusters: Number of clusters (K)\n",
    "- init: Initialization method ('random' or 'k-means++')\n",
    "- max_iter: Maximum iterations per run\n",
    "- random_state: Ensures reproducibility\n",
    "\"\"\"\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,          # We know there are 3 clusters (from make_blobs)\n",
    "    init='k-means++',      # Smart initialization (better than random)\n",
    "    max_iter=300,          # Maximum iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model and predict clusters\n",
    "kmeans.fit(X_scaled)\n",
    "labels = kmeans.predict(X_scaled)  # Cluster assignments (0, 1, 2)\n",
    "centroids = kmeans.cluster_centers_  # Coordinates of cluster centers\n",
    "\n",
    "print(\"\\nCluster Centers (Scaled Coordinates):\")\n",
    "print(centroids)\n",
    "\n",
    "# ======================================================================\n",
    "# 4. VISUALIZE CLUSTERS\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: VISUALIZE CLUSTERS\".center(70, '='))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot data points colored by cluster\n",
    "plt.scatter(\n",
    "    X_scaled[:, 0], X_scaled[:, 1],\n",
    "    c=labels,              # Color by cluster label\n",
    "    cmap='viridis',        # Color map for clusters\n",
    "    s=50,                  # Marker size\n",
    "    alpha=0.6             # Transparency\n",
    ")\n",
    "\n",
    "# Plot centroids\n",
    "plt.scatter(\n",
    "    centroids[:, 0], centroids[:, 1],\n",
    "    c='red',               # Color centroids red\n",
    "    marker='X',            # Use 'X' for centroids\n",
    "    s=200,                 # Larger size for visibility\n",
    "    label='Centroids'\n",
    ")\n",
    "\n",
    "plt.title(\"K-Means Clustering (K=3)\")\n",
    "plt.xlabel(\"Feature 1 (Standardized)\")\n",
    "plt.ylabel(\"Feature 2 (Standardized)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- Points are grouped into 3 distinct clusters.\n",
    "- Centroids (red X) represent the mean of each cluster.\n",
    "- K-Means successfully identified the underlying structure.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b0cc5-91ce-41b4-a83f-b82ec586f071",
   "metadata": {},
   "source": [
    "# **Program 2: Choosing the Optimal K (Elbow Method & Silhouette Analysis)**\n",
    "### **Objective**: Determine the best number of clusters (K) using two methods.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "DETERMINING OPTIMAL K IN K-MEANS\n",
    "================================\n",
    "Write a program to demonstrate the two methods for choosing K:\n",
    "1. The Elbow Method (inertia vs K)\n",
    "2. Silhouette Analysis (cluster cohesion vs separation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc172e-b95f-4515-97b2-9b07dd3c6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import SilhouetteVisualizer  # Requires `yellowbrick`\n",
    "\n",
    "# ======================================================================\n",
    "# 1. GENERATE DATA\n",
    "# ======================================================================\n",
    "print(\"STEP 1: GENERATE DATA\".center(70, '='))\n",
    "\n",
    "# Create a dataset with 4 clusters (but pretend we don't know this)\n",
    "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ======================================================================\n",
    "# 2. ELBOW METHOD\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 2: ELBOW METHOD\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "ELBOW METHOD:\n",
    "- Plot inertia (sum of squared distances to centroids) vs K.\n",
    "- Look for the \"elbow\" where inertia starts decreasing linearly.\n",
    "\"\"\"\n",
    "\n",
    "# Test K from 1 to 10\n",
    "K_range = range(1, 11)\n",
    "inertias = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)  # Store inertia for each K\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia (Within-Cluster Sum of Squares)\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.xticks(K_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- The \"elbow\" is at K=4 → optimal number of clusters.\n",
    "- Beyond K=4, adding more clusters doesn't significantly reduce inertia.\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# 3. SILHOUETTE ANALYSIS\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: SILHOUETTE ANALYSIS\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "SILHOUETTE SCORE:\n",
    "- Measures how similar a point is to its own cluster vs other clusters.\n",
    "- Range: [-1, 1] → Higher is better.\n",
    "\"\"\"\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, 11):  # Silhouette score requires at least 2 clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 11), silhouette_scores, 'go-')\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Analysis for Optimal K\")\n",
    "plt.xticks(range(2, 11))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- The highest silhouette score occurs at K=4 → best clustering structure.\n",
    "- Confirms the Elbow Method's suggestion.\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# 4. VISUALIZE SILHOUETTE FOR K=4 (OPTIMAL)\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: SILHOUETTE VISUALIZATION (K=4)\".center(70, '='))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "visualizer = SilhouetteVisualizer(KMeans(n_clusters=4, random_state=42))\n",
    "visualizer.fit(X_scaled)\n",
    "visualizer.show()\n",
    "\n",
    "\"\"\"\n",
    "SILHOUETTE PLOT EXPLANATION:\n",
    "- Each colored region represents a cluster.\n",
    "- Thickness = cluster size.\n",
    "- Dashed line = average silhouette score.\n",
    "- Ideally, all bars should be above average and similar in size.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72874357-e431-4b89-b238-58a865721634",
   "metadata": {},
   "source": [
    "## **Program 3 : K-Means on Iris Dataset**\n",
    "### **Objective**: Cluster iris flowers based on sepal/petal measurements.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "K-MEANS ON IRIS DATASET\n",
    "=======================\n",
    "\n",
    "1. Load the built-in Iris dataset\n",
    "2. Determine optimal K\n",
    "3. Visualize the results of clustering\n",
    "4. Compare clusters with actual species labels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c69002-5ae4-4f39-a675-be3b3f285cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ======================================================================\n",
    "# 1. LOAD IRIS DATA\n",
    "# ======================================================================\n",
    "print(\"STEP 1: LOAD DATA\".center(70, '='))\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "print(f\"\\nFeature Names: {feature_names}\")\n",
    "print(f\"Data Shape: {X.shape}\")\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# ======================================================================\n",
    "# 2. SCALE DATA & FIND OPTIMAL K\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 2: SCALE DATA & FIND OPTIMAL K\".center(70, '='))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Elbow Method\n",
    "inertias = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertias, 'bo-')\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- The elbow appears at K=3 (matches actual iris species count)\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# 3. APPLY K-MEANS (K=3)\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: APPLY K-MEANS (K=3)\".center(70, '='))\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add clusters to DataFrame\n",
    "df['Cluster'] = labels\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(df['Cluster'].value_counts())\n",
    "\n",
    "# ======================================================================\n",
    "# 4. VISUALIZE CLUSTERS (2D Projection)\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: VISUALIZE CLUSTERS\".center(70, '='))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "# Plot first two features (sepal length/width)\n",
    "for i in range(3):\n",
    "    plt.scatter(\n",
    "        X[labels == i, 0], X[labels == i, 1],\n",
    "        s=50, c=colors[i],\n",
    "        label=f'Cluster {i}'\n",
    "    )\n",
    "\n",
    "plt.scatter(\n",
    "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "    s=200, marker='X', c='black', label='Centroids'\n",
    ")\n",
    "\n",
    "plt.title(\"Iris Clustering (Sepal Features)\")\n",
    "plt.xlabel(\"Sepal Length (cm)\")\n",
    "plt.ylabel(\"Sepal Width (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- Clear separation between clusters in 2D space.\n",
    "- Compare with actual species (not shown here) for accuracy assessment.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d865e-d491-4a10-bebb-ce04b0ab37a4",
   "metadata": {},
   "source": [
    "\n",
    "## **Discussion Questions**\n",
    "1. **When does K-Means perform poorly?**  \n",
    "   - Non-globular clusters  \n",
    "   - Varying cluster densities  \n",
    "\n",
    "2. **How does scaling affect K-Means?**  \n",
    "   - Euclidean distance is scale-sensitive → always standardize features.  \n",
    "\n",
    "3. **Alternatives to K-Means?**  \n",
    "   - DBSCAN (density-based)  \n",
    "   - Hierarchical clustering  \n",
    "\n",
    "4. **Business Applications?**  \n",
    "   - Customer segmentation  \n",
    "   - Anomaly detection  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864b1f8-83f9-417e-8271-490e7e2082a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
