{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49156ffd-26ee-407b-8c3c-1227bfd7c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab7_knn_all.ipynb\n",
    "# 22 Jun 2025\n",
    "# Lab: k-Nearest Neighbors (KNN) for Classification and Regression\n",
    "# Dowload code from : https://github.com/svhari/CS_2225_Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa9f6e-b353-4b08-9f65-0654d963dad9",
   "metadata": {},
   "source": [
    "# **Lab 7: k-Nearest Neighbors (KNN) for Classification and Regression**\n",
    "\n",
    "## **Lab Objectives**\n",
    "1. Understand the **fundamentals of KNN** for classification and regression.\n",
    "2. Learn how to **choose optimal K** using cross-validation.\n",
    "3. Implement KNN in Python using `scikit-learn`.\n",
    "4. Visualize decision boundaries and performance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "# **Program 1: KNN Classification with Synthetic Data**\n",
    "### **Objective**: Implement KNN classification and visualize decision boundaries.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "KNN CLASSIFICATION DEMONSTRATION\n",
    "================================\n",
    "\n",
    "1. Generate synthetic classification data\n",
    "2. Train a KNN classifier\n",
    "3. Visualize decision boundaries\n",
    "4. Evaluate model performance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ba743-5405-48cc-ab6a-807ec0764ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# ======================================================================\n",
    "# 1. GENERATE SYNTHETIC DATA \n",
    "# ======================================================================\n",
    "print(\"STEP 1: GENERATE SYNTHETIC DATA\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "CORRECTION EXPLANATION:\n",
    "- Default n_informative=2, n_redundant=2 would require n_features >=4\n",
    "- For n_features=2, we must set n_informative <=2 and n_redundant=0\n",
    "\"\"\"\n",
    "# With `n_features=2`, the sum of informative+redundant+repeated must be ≤2\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=2,           # Only 2 features for visualization\n",
    "    n_informative=2,        # Both features are informative\n",
    "    n_redundant=0,          # No redundant features\n",
    "    n_repeated=0,           # No repeated features\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data (unchanged)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title(\"Synthetic Classification Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.colorbar(label=\"Class\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "# 2. TRAIN KNN CLASSIFIER\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 2: TRAIN KNN CLASSIFIER\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "KNN PARAMETERS:\n",
    "- n_neighbors: Number of neighbors to consider (K)\n",
    "- weights: 'uniform' (all neighbors equal) or 'distance' (weight by inverse distance)\n",
    "- p: Power parameter for Minkowski metric (1=Manhattan, 2=Euclidean)\n",
    "\"\"\"\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,    # Start with K=5\n",
    "    weights='uniform', # All neighbors contribute equally\n",
    "    p=2               # Euclidean distance\n",
    ")\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# ======================================================================\n",
    "# 3. EVALUATE PERFORMANCE\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: EVALUATE PERFORMANCE\".center(70, '='))\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- Diagonal elements show correct predictions\n",
    "- Off-diagonal shows misclassifications\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# 4. VISUALIZE DECISION BOUNDARIES\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: VISUALIZE DECISION BOUNDARIES\".center(70, '='))\n",
    "\n",
    "# Create a mesh grid for plotting\n",
    "h = 0.02  # Step size in mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict class for each mesh point\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(8, 6))\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=50)\n",
    "plt.title(\"KNN Decision Boundaries (K=5)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- The colored background shows the decision regions\n",
    "- Points show actual class labels\n",
    "- Observe how increasing K would smooth the boundary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f9beb-dc3d-4c99-bd88-e1df26a4fcbf",
   "metadata": {},
   "source": [
    "### Common Pitfalls to Avoid:\n",
    "1. **Default Parameters Trap**:\n",
    "   - `make_classification()` defaults assume `n_features≥4`\n",
    "   - Always check parameter interactions\n",
    "\n",
    "2. **Visualization Requirements**:\n",
    "   - For 2D plots, you must have exactly `n_features=2`\n",
    "   - For 3D plots, use `n_features=3`\n",
    "\n",
    "3. **Classification Complexity**:\n",
    "   - `flip_y` controls noise (keep ≤0.1 for clear separation)\n",
    "   - `class_sep` controls cluster separation (higher = easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712b693-bb14-47f9-b1f2-671ae9f8a6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a885b386-bfa6-4e20-9738-f81b6b59fcc6",
   "metadata": {},
   "source": [
    "# **Program 2: Finding Optimal K with Cross-Validation**\n",
    "### **Objective**: Determine the best K value using cross-validation.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "FINDING OPTIMAL K FOR KNN\n",
    "=========================\n",
    "This program demonstrates:\n",
    "1. Using cross-validation to find optimal K\n",
    "2. Analyzing bias-variance tradeoff\n",
    "3. Visualizing accuracy vs K\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fff2ec-e19a-4273-8cc3-d1198b1f7fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ======================================================================\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# ======================================================================\n",
    "print(\"STEP 1: LOAD DATA\".center(70, '='))\n",
    "\n",
    "# Load Wisconsin Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "print(f\"\\nDataset shape: {X.shape}\")\n",
    "print(f\"Feature names: {feature_names[:5]}...\")  # Show first 5 features\n",
    "\n",
    "# Standardize features (critical for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ======================================================================\n",
    "# 2. FIND OPTIMAL K WITH CROSS-VALIDATION\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 2: FIND OPTIMAL K\".center(70, '='))\n",
    "\n",
    "\"\"\"\n",
    "WHY CROSS-VALIDATION?\n",
    "- More reliable than single train/test split\n",
    "- Reduces variance in performance estimation\n",
    "\"\"\"\n",
    "\n",
    "# Test K values from 1 to 30\n",
    "k_range = range(1, 31)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_scaled, y, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find K with highest accuracy\n",
    "optimal_k = k_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nOptimal K: {optimal_k}\")\n",
    "print(f\"Highest CV Accuracy: {max(cv_scores):.3f}\")\n",
    "\n",
    "# ======================================================================\n",
    "# 3. VISUALIZE ACCURACY VS K\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: VISUALIZE K SELECTION\".center(70, '='))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, cv_scores, 'bo-')\n",
    "plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal K={optimal_k}')\n",
    "plt.xlabel(\"Number of Neighbors (K)\")\n",
    "plt.ylabel(\"Cross-Validated Accuracy\")\n",
    "plt.title(\"KNN Performance vs K Value\")\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- Small K: High variance (overfitting)\n",
    "- Large K: High bias (underfitting)\n",
    "- Optimal K balances both (usually in middle range)\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# 4. TRAIN FINAL MODEL WITH OPTIMAL K\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: FINAL MODEL\".center(70, '='))\n",
    "\n",
    "final_knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "final_knn.fit(X_scaled, y)\n",
    "\n",
    "# Feature importance (based on permutation importance)\n",
    "print(\"\\nModel trained with optimal K:\")\n",
    "print(f\"K = {optimal_k}, Metric = Euclidean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68fd8e-7f17-49c4-971d-3b9a5c13988a",
   "metadata": {},
   "source": [
    "# **Program 3: KNN Regression**\n",
    "### **Objective**: Implement KNN for regression tasks.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "KNN REGRESSION DEMONSTRATION\n",
    "============================\n",
    "This program demonstrates:\n",
    "1. Using KNN for regression\n",
    "2. Comparing different K values\n",
    "3. Evaluating with RMSE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd2fa4-caef-4b2e-8ca5-55bc75cd257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ======================================================================\n",
    "# 1. GENERATE REGRESSION DATA\n",
    "# ======================================================================\n",
    "print(\"STEP 1: GENERATE DATA\".center(70, '='))\n",
    "\n",
    "# Create synthetic regression data with some noise\n",
    "X, y = make_regression(\n",
    "    n_samples=300, \n",
    "    n_features=1,          # Single feature for visualization\n",
    "    noise=20,              # Add realistic noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X, y, color='blue', alpha=0.6)\n",
    "plt.title(\"Synthetic Regression Data\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "# 2. TRAIN KNN REGRESSOR\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 2: TRAIN KNN REGRESSOR\".center(70, '='))\n",
    "\n",
    "knn_reg = KNeighborsRegressor(\n",
    "    n_neighbors=5,\n",
    "    weights='distance'  # Closer points have more influence\n",
    ")\n",
    "\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# ======================================================================\n",
    "# 3. EVALUATE PERFORMANCE\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 3: EVALUATE MODEL\".center(70, '='))\n",
    "\n",
    "y_pred = knn_reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"\\nRoot Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# ======================================================================\n",
    "# 4. VISUALIZE PREDICTIONS\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 4: VISUALIZE RESULTS\".center(70, '='))\n",
    "\n",
    "# Create test points for smooth curve visualization\n",
    "X_plot = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)\n",
    "y_plot = knn_reg.predict(X_plot)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='green', label='Test Data')\n",
    "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='KNN Prediction')\n",
    "plt.title(\"KNN Regression (K=5)\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "INTERPRETATION:\n",
    "- The red line shows KNN's piecewise constant predictions\n",
    "- Smaller K would make the curve more wiggly (overfitting)\n",
    "- Larger K would make it smoother (underfitting)\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# 5. COMPARE DIFFERENT K VALUES\n",
    "# ======================================================================\n",
    "print(\"\\nSTEP 5: COMPARE K VALUES\".center(70, '='))\n",
    "\n",
    "k_values = [1, 5, 15, 30]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, k in enumerate(k_values, 1):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_plot = knn.predict(X_plot)\n",
    "    \n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(X_train, y_train, color='blue', alpha=0.3)\n",
    "    plt.plot(X_plot, y_plot, color='red', linewidth=2)\n",
    "    plt.title(f\"KNN Regression (K={k})\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa130c-5b71-4182-9edb-f33f67801de9",
   "metadata": {},
   "source": [
    "\n",
    "# **Optional Exercises**\n",
    "1. **Decision Boundary Analysis**  \n",
    "   - In Program 1, test `K=1` and `K=50`. How do decision boundaries change?  \n",
    "   - Which K shows signs of overfitting/underfitting?  \n",
    "\n",
    "2. **Feature Scaling Impact**  \n",
    "   - Run Program 2 without standardization. How does accuracy change?  \n",
    "\n",
    "3. **Distance Metrics**  \n",
    "   - Compare Manhattan (`p=1`) vs Euclidean (`p=2`) distance in Program 1.  \n",
    "\n",
    "4. **Real-World Application**  \n",
    "   - Apply KNN to the Iris dataset for classification. Compare with K-Means clusters.  \n",
    "\n",
    "---\n",
    "\n",
    "# **Discussion Questions**\n",
    "1. **When does KNN perform poorly?**  \n",
    "   - High-dimensional data (curse of dimensionality)  \n",
    "   - Imbalanced datasets  \n",
    "\n",
    "2. **How does K affect bias/variance?**  \n",
    "   - Small K → Low bias, high variance  \n",
    "   - Large K → High bias, low variance  \n",
    "\n",
    "3. **Alternatives to KNN?**  \n",
    "   - Decision trees (for non-metric data)  \n",
    "   - SVM (for high-dimensional spaces)  \n",
    "\n",
    "4. **Business Applications?**  \n",
    "   - Recommendation systems  \n",
    "   - Anomaly detection  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2c873-6527-42c6-9afc-ecca94ee5bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
